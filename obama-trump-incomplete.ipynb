{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Obama/Trump Tweet Classifier\n",
    "\n",
    "For this workshop, you will be training a Recurrent Neural Network to classify between Obama's tweets and Trump's tweets. You will need to be familiar with the theory behind RNNs and/or LSTMs before starting. Visit our lesson [here](http://caisplusplus.usc.edu/blog/curriculum/lesson8) for more info. Only fill in the TODO sections!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load the Data\n",
    "1. Load in Tweets and corresponding labels (Obama = 0, Trump = 1)\n",
    "2. Display some sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TWEETS_DIR = './obama-trump-data/tweets'\n",
    "LABELS_DIR = './obama-trump-data/labels_np'\n",
    "\n",
    "######################################\n",
    "# TODO: specify your own word embeddings file directory here\n",
    "EMBEDDINGS_DIR = None\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  \"America should be very proud.\" President Obama #LoveWins . Label:  0\n",
      "Tweet:  TO ALL AMERICANS\n",
      "https://t.co/D7Es6ie4fY . Label:  1\n",
      "Tweet:  No deal was made last night on DACA. Massive border security would have to be agreed to in exchange for consent. Would be subject to vote. . Label:  1\n",
      "Tweet:  I was viciously attacked by Mr. Khan at the Democratic Convention. Am I not allowed to respond? Hillary voted for the Iraq war, not me! . Label:  1\n",
      "Tweet:  .@RudyGiuliani, one of the finest people I know and a former GREAT Mayor of N.Y.C., just took himself out of consideration for \"State\". . Label:  1\n",
      "Tweet:  The #TPP establishes the highest labor standards of any trade agreement in history. http://t.co/3vqoancyYp . Label:  0\n",
      "Tweet:  Thoughts and prayers to the great people of Indiana. You will prevail! . Label:  1\n",
      "Tweet:  Hillary said she was under sniper fire (while surrounded by USSS.) Turned out to be a total lie. She is not fit to https://t.co/hBIrGj21l6 . Label:  1\n",
      "Tweet:  The failing @nytimes has become a newspaper of fiction. Their stories about me always quote non-existent unnamed sources. Very dishonest! . Label:  1\n",
      "Tweet:  Thank you Council Bluffs, Iowa! Will be back soon. Remember- everything you need to know about Hillary -- just https://t.co/45kIHxdX83 . Label:  1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load tweets, labels\n",
    "tweets = pickle.load(open(TWEETS_DIR,'rb'))\n",
    "labels = pickle.load(open(LABELS_DIR,'rb'))\n",
    "\n",
    "# Sample some tweets to display\n",
    "for i in range(0,100,10):\n",
    "    print \"Tweet: \", tweets[i], \". Label: \", labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "Relevant Keras documentation [here](https://keras.io/preprocessing/text/).\n",
    "\n",
    "1. **Tokenize** the tweets: convert each tweet into a sequence of word indices (Documentation .)\n",
    "2. **Pad** the input sequences with blank spots at the end to make sure they're all the same length\n",
    "3. Load in pre-trained word embeddings so that we can convert each word index into a unique **word embedding vector**\n",
    "\n",
    "**Word embeddings** are a way of encoding words into n-dimensional vectors with continuous values so that the vector contains some information about the words' meanings. **Embedded word vectors** are often more useful than **one-hot vectors** in natural language processing applications, because the vectors themselves contain some embedded information about the word's meaning, instead of just identifying which word is there. \n",
    "\n",
    "For example, similar words (e.g. \"him\" and \"her\") tend to have similar embedded vectors, whereas if we were just using one-hot vectors (i.e. only one \"1\" for \"him\" and one \"1\" for \"her\"), no notion of the words' actual meanings would be conveyed. Although this may sound crazy at first, word embeddings can even convey relationships between analogous words: for example: `king-man+womanâ‰ˆqueen`.\n",
    "\n",
    "For more info on word embeddings, check out this introductory [blog post](https://www.springboard.com/blog/introduction-word-embeddings/). Two of the most common word embeddings methods are [word2vec](https://deeplearning4j.org/word2vec.html) and [GloVe](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13049 unique tokens (words).\n",
      "Training data size is (6136,31)\n",
      "Labels are size 6136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the tweets (convert sentence to sequence of words)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens (words).' % len(word_index))\n",
    "\n",
    "# Pad sequences to ensure samples are the same size\n",
    "training_data = pad_sequences(sequences)\n",
    "\n",
    "print(\"Training data size is (%d,%d)\"  % training_data.shape) # shape = (num. tweets, max. length of tweet)\n",
    "print(\"Labels are size %d\"  % labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:  \"America should be very proud.\" President Obama #LoveWins\n",
      "Tweet after tokenization and padding:  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  44  88  21  83 391  12  14 794]\n"
     ]
    }
   ],
   "source": [
    "# Show effect of tokenization, padding\n",
    "print \"Original tweet: \", tweets[0]\n",
    "print \"Tweet after tokenization and padding: \", training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert words to word embedding vectors\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDINGS_DIR) # NOTE: if using Windows and getting errors: change this to open(EMBEDDINGS_DIR, 'rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] # NOTE: if you made that 'rb' change: add to this line, values[0].decode('UTF-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# prepare word embedding matrix\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample word embedding vector:\n",
    "print(word_index[\"computer\"]) # retrieve word index\n",
    "print(embedding_matrix[2645]) # use word index to retrieve word embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Classifier Model\n",
    "Here are some useful resources to help you guys out here:\n",
    "\n",
    "#### Keras Documentation\n",
    "* [Embedding layer](https://keras.io/layers/embeddings/) (our word embeddings are pre-trained, so you won't have to worry too much about this)\n",
    "* [**Recurrent layers (e.g. RNN, LSTM)**](https://keras.io/layers/recurrent/) (Feel free to play around with different layers.)\n",
    "* [Dense layer](https://keras.io/layers/core/#dense) (Use this for your final classification \"vote\")\n",
    "* [Activation functions](https://keras.io/activations/)\n",
    "* [Dropout](https://keras.io/layers/core/#dropout), [Batch Normalization](https://keras.io/layers/normalization/)\n",
    "* [Optimizers](https://keras.io/optimizers/)\n",
    "\n",
    "#### Examples\n",
    "* [Keras sequential model guide](https://keras.io/getting-started/sequential-model-guide/) (Scroll down to \"Sequence classification with LSTM\")\n",
    "* [Keras LSTM for IMDB review sentiment analysis](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)\n",
    "\n",
    "**Hint**: if you have multiple recurrent layers, remember to use `return_sequences=True` if and only if you're adding another recurrent layer after the current one. This makes it so that the recurrent layer spits out an output after each timestep (or element in the sequence), instead of just at the very end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers import Dropout, concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras import metrics\n",
    "from keras.models import Model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add pre-trained embedding layer \n",
    "    # converts word indices to GloVe word embedding vectors as they're fed in\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=training_data.shape[1],\n",
    "                    trainable=False))\n",
    "\n",
    "# At this point, each individual training sample is now a sequence of word embedding vectors\n",
    "\n",
    "######################################\n",
    "# TODO: define the rest of the network!\n",
    "\n",
    "\n",
    "######################################\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "######################################\n",
    "# TODO: pick out a loss function and corresponding optimizer.\n",
    "    # Hint: there's one \"correct\" loss function we have in mind,\n",
    "        # since we're just classifying between 0 and 1.\n",
    "    # You can pick your own optimizer, but Keras's documentation\n",
    "        # says that RMSprop typically works better for recurrent neural networks.\n",
    "LOSS = None\n",
    "OPTIMIZER = None\n",
    "######################################\n",
    "\n",
    "\n",
    "model.compile(loss = LOSS, optimizer = OPTIMIZER, metrics = [metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Evaluate the Model\n",
    "Train the network. Look to make sure that the loss is decreasing and the accuracy is decreasing, but be on the lookout for overfitting. \n",
    "\n",
    "Aim for **90%** final validation accuracy! (Pretty much the same as using a separate test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# TODO: pick number of epochs and batch size\n",
    "\n",
    "EPOCHS = None\n",
    "BATCH_SIZE = None\n",
    "#####################################\n",
    "\n",
    "model.fit(training_data, labels, \n",
    "          epochs = EPOCHS, \n",
    "          batch_size = BATCH_SIZE, \n",
    "          validation_split =.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
